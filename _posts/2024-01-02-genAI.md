---
title: Generative AI with its development
author: Fangzheng
date: 2024-01-02 10:06:00 +0800
categories: [Generative AI, Artificial Intelligence]
tags: [Algorithm ]
# pin: true
mermaid: true  #code模块
comments: true

math: true
# img_cdn: https://github.com/MysteriousL2019/mysteriousl2019.github.io/tree/master/assets/img/
---
## Background
* 一般而言，深度学习模型可以分为判别式模型（Discriminative model）与生成式模型（Generative model）。判别模型需要输入变量，通过
某种模型来预测。生成模型是给定某种隐含信息，来随机产生观测数据。举个简单的例子，
* 判别模型：给定一张图，判定图中的动物是什么类别
* 生成模型：给一系列猫的图片，生成一张新的猫咪
* 由于反向传播 (Back propagation, BP) 、 Dropout等算法的发明，判别式模型得到了迅速发展。然而，由于生成式模型建模较为困难，因此发展缓慢， 直到近年来最成功的生成模型——生成式对抗网络的发明，这一领域才焕发新的生机。
### P.S. with contrastive learning
* 对比式学习着重于学习同类实例之间的共同特征，区分非同类实例之间的不同之处。
* 与生成式学习比较，对比式学习不需要关注实例上繁琐的细节，只需要在抽象语义级别的特征空间上学会对数据的区分即可，因此模型以及其优化变得更加简单，且泛化能力更强。
* 对比学习的目标是学习一个编码器，此编码器对同类数据进行相似的编码，并使不同类的数据的编码结果尽可能的不同。
### Serval Generative models
* GAN
* VAE
* Diffusion model
## CLIP(Contrastive Language-Image Pre-training)
* zero-shot learning:
* few-shot learning(少样本学习):众所周知，现在的主流的传统深度学习技术需要大量的数据来训练一个好的模型。例如典型的 MNIST 分类问题，一共有 10 个类，训练集一共有 6000 个样本，平均下来每个类大约 600 个样本，但是我们想一下我们人类自己，我们区分 0 到 9 的数字图片的时候需要看 6000 张图片才知道怎么区分吗？很显然，不需要！这表明当前的深度学习技术和我们人类智能差距还是很大的，要想弥补这一差距，少样本学习是一个很关键的问题。另外还有一个重要原因是如果想要构建新的数据集，还是举分类数据集为例，我们需要标记大量的数据，但是有的时候标记数据集需要某些领域的专家（例如医学图像的标记），这费时又费力，因此如果我们可以解决少样本学习问题，只需要每个类标记几张图片就可以高准确率的给剩余大量图片自动标记。这两方面的原因都让少样本学习问题很吸引人。在 few-shot learning 中有一个术语叫做 N NN-way K KK-shot 问题，简单的说就是我们需要分类的样本属于 N NN 个类中一种，但是我们每个类训练集中的样本只有 K KK 个，即一共只有 N ∗ K N * KN∗K 个样本的类别是已知的。
### 解决方法  
* 数据增强和正则化： 这一类方法想法很直接简单，既然训练数据不够那我就增加训练样本，既然过拟合那我就使用正则化技术。
* meta-learning:元学习的核心想法是先学习一个先验知识（prior），这个先验知识对解决 few-shot learning 问题特别有帮助。Meta-learning 中有 task 的概念，比如上面图片讲的 5-way 1-shot 问题就是一个 task，我们需要先学习很多很多这样的 task，然后再来解决这个新的 task 。最最最重要的一点，这是一个新的 task。分类问题中，这个新的 task 中的类别是之前我们学习过的 task 中没有见过的！ 在 Meta-learning 中之前学习的 task 我们称为 meta-training task，我们遇到的新的 task 称为 meta-testing task。因为每一个 task 都有自己的训练集和测试集，因此为了不引起混淆，我们把 task 内部的训练集和测试集一般称为 support set 和 query set
* 方法 2.1 和方法 2.2 之间有个明显的差异就是 meta-learning 需要一些类来构建 meta-training task。由于 meta-testing 的类别要和 meta-training 完全不同，因此如果我们只有 MNIST 数据集，没法使用 meta-learning 来解决 MNIST 上的 10-way few-shot learning 问题，但是方法 2.1 可以。不过我们可以使用 meta-learning 解决 MNIST 上的 N-way (N < 6) 的 few-shot learning 问题。那么如果我们非要解决 MNIST 上的 10-way few-shot learning 问题怎么办呢，可以在另外一个数据集，例如 Omniglot ，上面进行 meta-training，然后学到的先验知识用来解决这个问题。《Siamese neural networks for one-shot image recognition.》 和 《Matching networks for one shot learning.》都在论文中做过这样子实验（这一种情况和迁移学习（transfer learning）有相似的地方）。
<!-- *  -->
### Reference 
* [Details of CLIP](https://mysteriousl2019.github.io/posts/CLIP/)