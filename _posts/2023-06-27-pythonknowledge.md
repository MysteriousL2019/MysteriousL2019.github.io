---
title: Pytorch Tricks
author: Fangzheng
date: 2023-06-27 9:16:00 +0800
categories: [machine learning]
tags: [hint]
# pin: true
mermaid: true  #code模块
# comments: true
math: true
# img_cdn: https://github.com/MysteriousL2019/mysteriousl2019.github.io/tree/master/assets/img/
---
## 为何将batch size设置为1？
* 通常的项目为了将GPU的算力充分利用，会将batch size设置为一个较大的值
* 在pytorch version中关于目标检测的训练脚本，将batch size设置为了1是因为： 不同尺寸的图片打成同一个batch送到网络之前必然要resize（比如padding）成同一个尺寸，为了避免resize带来的干预，所以让一个batch只有一个图片是最好的（前提是你是用的数据尺寸不同，如果相同的话，当然大点好，跑满）

## 训练需要热身？（warm-up）
###  通常用于深度学习中，特别是在训练神经网络时。Warmup是指在训练的初期阶段逐渐增加学习率或其他超参数的过程，目的是帮助网络更快地收敛到一个良好的状态，减少训练过程中的不稳定性。下面是一些关于训练warmup的常见做法：
* * 1. 学习率的Warmup：
* 在深度学习中，学习率是一个关键的超参数，它控制了权重更新的步长。通常，在训练的初期，学习率会设置得比较小，然后随着训练的进行逐渐增加。这个过程被称为学习率的Warmup。逐渐增加学习率有助于防止模型在训练初期因为学习率过大而发生不稳定的情况。

* * 2. 批量大小（Batch Size）的Warmup
* 在一些情况下，逐渐增加批量大小也可以被认为是一种Warmup策略。初始阶段使用较小的批量大小，然后逐渐增加，以减小训练过程的不稳定性。这对于某些模型或任务可能会有帮助。
    * 减小训练不稳定性：在训练初期使用较小的批量大小有助于减小模型参数更新的随机性，避免由于参数更新过快或者过慢导致的训练不稳定问题。逐渐增加批量大小可以使得模型在训练过程中逐渐适应更大的批量大小，减小训练的不稳定性。

    * 控制内存和计算资源：较小的批量大小通常会导致更频繁的参数更新，这可能会增加内存和计算资源的消耗。逐渐增加批量大小可以在训练过程中逐步调整参数更新的频率，更好地利用计算资源，并确保训练过程的高效性。

    * 更好的泛化性能：逐步增加批量大小可能有助于提高模型的泛化性能。较小的批量大小可以促使模型更加细致地学习数据的特征，而较大的批量大小则可以提高模型的泛化能力，降低过拟合的风险。

    * 更快的收敛速度：逐步增加批量大小可以加快训练的收敛速度。在训练初期使用较小的批量大小可以使得模型更快地收敛到局部最优解附近，然后逐步增加批量大小可以使得模型更快地朝着全局最优解收敛。

* * 3. 层（Layer）的Warmup
* 在一些大型神经网络中，逐渐解冻（或解除冻结）层也可以被认为是Warmup的一部分。例如，对于迁移学习（Transfer Learning），你可以首先冻结大部分网络层，只训练少量的层，然后逐渐解冻更多的层，以帮助网络适应新的任务。

* * 4. 数据的Warmup
* 有时候，数据集可能包含一些噪声或不一致性。在训练初期，可以使用较小的数据子集进行Warmup，以便在稳定之前尽早发现和解决数据问题。
* * 5. Loss的Warmup
* 在一些情况下，为了避免初始阶段的训练不稳定性，可以使用一个较小的Loss权重或使用更简单的Loss函数，然后逐渐增加Loss的权重或切换到更复杂的Loss函数。
* Warmup策略的具体实施取决于模型、数据和任务。它可以帮助网络更快地收敛，减少训练过程中的震荡，但也需要根据具体情况进行调整和优化。
* 在训练初期，lr*warm-up（非常小的值），之后lr升高后，再通过正常训练下降。
* 在训练刚开始的时候，训练参数是random的，可能离最终的目标参数差距过大，如果不设置warm-up可能会导致训练不稳定
* 初次提出是resnet
## 为何保存的权重文件这么大？
* 预训练的参数比自己训练的保存小很多
* 因为仅仅保存了model的权重，没有保存optimizer, lr, epoch

## pytorch Tensor中通道排列顺序
* [batch, channel, height, width]
* 一批的图片个数，通道数，高，宽

## 利用BP进行车牌分类
* 输入的RGB图像进行灰度化-》二值化 -- 因为结果只关注形状，与颜色无关
* 进行（滑动窗口），本质计算白色色块占卷积区域的比例
* 将卷积结果变为行向量，输入神经网络
* 使用one-hot编码输出层编码 完成25维 -> 10维的变化

# sklearn.model_selection的train_test_split
* X_train, X_test = train_test_split(X,test_size=0.3)
* y_train, y_test = train_test_split(y, test_size=0.3)
* X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
* 第一种写法分别对X和y进行了train_test_split，并且使用相同的test_size参数。这意味着，X和y中对应的样本会被分到相同的数据集中，但是它们的顺序可能不同，因为train_test_split函数默认会对数据进行随机打乱。因此，在第一种写法中，需要在后面手动将X_train和y_train以及X_test和y_test进行组合，保证它们的顺序一致。
* 而第二种写法直接对X和y同时进行train_test_split，并将结果赋值给四个变量。这样可以保证X_train与其对应的标签y_train，以及X_test与其对应的标签y_test是一一对应的，它们的顺序也是相同的。因此，第二种写法更加简洁易懂，推荐使用。
